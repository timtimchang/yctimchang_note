<!DOCTYPE html><html><head>
      <title>note</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      <link rel="stylesheet" href="file:///c:\Users\ESB21549\.vscode\extensions\shd101wyy.markdown-preview-enhanced-0.6.2\node_modules\@shd101wyy\mume\dependencies\katex\katex.min.css">
      
      
      
      
      
      
      
      
      
      <style>
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}

/* highlight */
pre[data-line] {
  position: relative;
  padding: 1em 0 1em 3em;
}
pre[data-line] .line-highlight-wrapper {
  position: absolute;
  top: 0;
  left: 0;
  background-color: transparent;
  display: block;
  width: 100%;
}

pre[data-line] .line-highlight {
  position: absolute;
  left: 0;
  right: 0;
  padding: inherit 0;
  margin-top: 1em;
  background: hsla(24, 20%, 50%,.08);
  background: linear-gradient(to right, hsla(24, 20%, 50%,.1) 70%, hsla(24, 20%, 50%,0));
  pointer-events: none;
  line-height: inherit;
  white-space: pre;
}

pre[data-line] .line-highlight:before, 
pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-start);
  position: absolute;
  top: .4em;
  left: .6em;
  min-width: 1em;
  padding: 0 .5em;
  background-color: hsla(24, 20%, 50%,.4);
  color: hsl(24, 20%, 95%);
  font: bold 65%/1.5 sans-serif;
  text-align: center;
  vertical-align: .3em;
  border-radius: 999px;
  text-shadow: none;
  box-shadow: 0 1px white;
}

pre[data-line] .line-highlight[data-end]:after {
  content: attr(data-end);
  top: auto;
  bottom: .4em;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;background-color:#f0f0f0;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}.markdown-preview{width:100%;height:100%;box-sizing:border-box}.markdown-preview .pagebreak,.markdown-preview .newpage{page-break-before:always}.markdown-preview pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}.markdown-preview pre.line-numbers>code{position:relative}.markdown-preview pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.markdown-preview pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}.markdown-preview pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}.markdown-preview .mathjax-exps .MathJax_Display{text-align:center !important}.markdown-preview:not([for="preview"]) .code-chunk .btn-group{display:none}.markdown-preview:not([for="preview"]) .code-chunk .status{display:none}.markdown-preview:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px}.scrollbar-style::-webkit-scrollbar{width:8px}.scrollbar-style::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}.scrollbar-style::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode]){position:relative;width:100%;height:100%;top:0;left:0;margin:0;padding:0;overflow:auto}html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{position:relative;top:0}@media screen and (min-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em calc(50% - 457px + 2em)}}@media screen and (max-width:914px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode]) .markdown-preview{font-size:14px !important;padding:1em}}@media print{html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{display:none}}html body[for="html-export"]:not([data-presentation-mode]) #sidebar-toc-btn{position:fixed;bottom:8px;left:8px;font-size:28px;cursor:pointer;color:inherit;z-index:99;width:32px;text-align:center;opacity:.4}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] #sidebar-toc-btn{opacity:1}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc{position:fixed;top:0;left:0;width:300px;height:100%;padding:32px 0 48px 0;font-size:14px;box-shadow:0 0 4px rgba(150,150,150,0.33);box-sizing:border-box;overflow:auto;background-color:inherit}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar{width:8px}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-track{border-radius:10px;background-color:transparent}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc::-webkit-scrollbar-thumb{border-radius:5px;background-color:rgba(150,150,150,0.66);border:4px solid rgba(150,150,150,0.66);background-clip:content-box}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc a{text-decoration:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{padding:0 1.6em;margin-top:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc li{margin-bottom:.8em}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .md-sidebar-toc ul{list-style-type:none}html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{left:300px;width:calc(100% -  300px);padding:2em calc(50% - 457px -  150px);margin:0;box-sizing:border-box}@media screen and (max-width:1274px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{padding:2em}}@media screen and (max-width:450px){html body[for="html-export"]:not([data-presentation-mode])[html-show-sidebar-toc] .markdown-preview{width:100%}}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .markdown-preview{left:50%;transform:translateX(-50%)}html body[for="html-export"]:not([data-presentation-mode]):not([html-show-sidebar-toc]) .md-sidebar-toc{display:none}
/* Please visit the URL below for more information: */
/*   https://shd101wyy.github.io/markdown-preview-enhanced/#/customize-css */

      </style>
    </head>
    <body for="html-export">
      <div class="mume markdown-preview  ">
      <h1 class="mume-header" id="cascadetabnet-an-approach-for-end-to-end-table-detection-and-structure-recognition-from-image-based-documents">CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents</h1>

<p>ref: <a href="https://github.com/DevashishPrasad/CascadeTabNet/blob/master/README.md">https://github.com/DevashishPrasad/CascadeTabNet/blob/master/README.md</a></p>
<h6 class="mume-header" id="devashish-prasad-ayan-gadpal-kshitij-kapadni-manish-visave-kavita-sultanpure-cvpr2020">Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, Kavita Sultanpure, CVPR2020</h6>

<h2 class="mume-header" id="abstract">Abstract</h2>

<ul>
<li>
<p>In this paper, we present an improved deep learning-based end to end approach for solving both problems of <strong>table detection</strong> and <strong>structure recognition</strong> using a single Convolution Neural Network (CNN) model.</p>
</li>
<li>
<p>We propose CascadeTabNet: a Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet) based model that <strong>detects the regions of tables</strong> and <strong>recognizes the structural body cells from the detected tables</strong> at the same time.</p>
</li>
</ul>
<p><img src="2022-03-23-14-44-32.png" alt></p>
<h2 class="mume-header" id="cascadetabnet-the-presented-approach">CascadeTabNet: The presented approach</h2>

<p>We try to focus on using a small amount of data effectively to achieve high accuracy results. Working towards this goal, our primary strategy includes:</p>
<ul>
<li>Using a <strong>relatively complex but efficient CNN architecture</strong> that attains high accuracy on object detection and segmentation benchmarking datasets as the main component in the approach.</li>
<li>Using an <strong>iterative transfer learning approach</strong> to train the CNN model gradually, starting from more general tasks and going towards more specific tasks. Performing iterations of transfer learning multiple times to extract the needful knowledge effectively from a small amount of data.</li>
<li>Strengthening the learning process by applying image transformation techniques to training images for <strong>data augmentation</strong>.</li>
</ul>
<h3 class="mume-header" id="model-architecture">Model architecture</h3>

<p><img src="2022-03-23-10-50-33.png" alt></p>
<ul>
<li>
<p><strong>Cascade RCNN</strong></p>
</li>
<li>
<p><strong>HRNetV2p</strong>: The original architecture of HRNet was enhanced for semantic segmentation to form HRNetV2. And, then a feature pyramid was formed over HRNetV2 for object detection to form HRNetV2p.</p>
</li>
</ul>
<p>CascadeTabNet is a three-staged Cascade mask R-CNN HRNet model. A backbone such as a ResNet-50 without the last fully connected layer is a part of the model that transforms an image to feature maps.</p>
<ul>
<li>As shown in figure 1, the image <strong>&#x201D;I&#x201D;</strong> is fed into the model. The backbone CNN HR NetV2p W32 transforms the image &#x201D;I&#x201D; to feature maps.</li>
<li>The <strong>&#x201D;RPN Head&#x201D;</strong> (Dense Head) predicts the preliminary object proposals for these feature maps.</li>
<li>The <strong>&#x201D;Bbox Heads&#x201D;</strong> take RoI features as input and make RoI-wise predictions.</li>
<li>Each head makes two predictions as <strong>bounding box classification scores</strong> and <strong>box regression points</strong>.</li>
<li><strong>&#x201D;B&#x201D;</strong> denotes the bounding boxes predicted by the heads and, for simplicity, we do not show the classification scores in the figure.</li>
<li>The <strong>&#x201D;Mask Head&#x201D;</strong> predicts the masks for the objects and <strong>&#x201D;S&#x201D;</strong> denotes a segmentation output. At the inference, object detections made by &#x201D;Bbox Heads&#x201D; are complemented with segmentation masks made by &#x201D;Mask Head&#x201D;, for all detected objects.</li>
</ul>
<h3 class="mume-header" id="iterative-transfer-learning">Iterative transfer learning</h3>

<p><img src="2022-03-23-17-41-17.png" alt></p>
<ol>
<li>
<p>The <strong>two-stage transfer learning strategy</strong> is used to make a single model learn end to end table recognition using a small amount of data.</p>
<ul>
<li>
<p>in the first iteration of transfer learning, we initialize our CNN model with the <strong>pre-trained imagenet coco model weights</strong> before training. After training, CNN successfully predicts the table detection masks for <strong>tables</strong> in the images.</p>
</li>
<li>
<p>in the second iteration, the model is again <strong>fine-tuned on a smaller dataset</strong> to accomplish even more specific task of predicting the <strong>cell</strong> masks in borderless tables along with detecting tables according to their types.</p>
</li>
</ul>
</li>
<li>
<p>We create a <strong>general dataset</strong> for a general task of table detection. We add images of different types of documents like word and latex in this dataset. These documents contain tables of various types like bordered, semi-bordered and borderless.</p>
<ul>
<li>A <strong>bordered</strong> table is one for which an algorithm can use just the line positions to estimate the cells and overall structure of the table.</li>
<li>If some of the lines are missing, it becomes difficult for a line detection based algorithm to separate the adjacent cells of the table. We call such a table as a <strong>semi-bordered</strong> table, in which some lines are not present.</li>
<li>And a <strong>borderless</strong> table is one which doesn&#x2019;t have any lines.</li>
</ul>
</li>
<li>
<p>This ew dataset contains slightly advanced annotations intimating the model to detect tables of two types with their labels (two classes) as <strong>bordered</strong>  and <strong>borderless</strong> (with borderless and semi-bordered), as well as predict borderless table cell masks (total three classes).</p>
</li>
</ol>
<h2 class="mume-header" id="pipeline">Pipeline</h2>

<p><img src="2022-03-23-11-24-21.png" alt></p>
<p>In the <strong>bordered branch</strong>,</p>
<ul>
<li>A <strong>conventional algorithm of line detection</strong> is used to detect lines of bordered tables.</li>
<li>The cells are identified using the <strong>line intersection points</strong>.</li>
<li>And within each cell, the text regions are detected by using the <strong>contourbased text detection algorithm</strong>.</li>
</ul>
<p>In the <strong>borderless branch</strong>,</p>
<ul>
<li>We arrange the predicted cells detected inside the table into <strong>rows</strong> and <strong>columns</strong> based on their positions.</li>
<li>We <strong>estimate the missing table lines</strong> using the positions of identified rows and columns.</li>
<li>Based on these lines, for undetected cells, we detect cells using a <strong>contourbased text detection algorithm</strong>.</li>
</ul>
<p><img src="2022-03-23-14-46-34.png" alt></p>
<h2 class="mume-header" id="image-transformation-and-data-augmentation">Image Transformation and data augmentation</h2>

<ul>
<li>Dilation transform</li>
<li>Smudge transform</li>
</ul>
<p><img src="2022-03-23-11-19-07.png" alt></p>
<h2 class="mume-header" id="results-and-analysis">Results and Analysis</h2>

<p>For creating a General dataset for table detection task we merge three datasets of ICDAR 19 (cTDaR), Marmot and Github 1.</p>
<h3 class="mume-header" id="preliminary-analysis">Preliminary Analysis</h3>

<p>Evaluation metrics for ICDAR 19 dataset are based on IoU (Intersection over Union) to evaluate the performance of <strong>table region detection</strong>. These results proved that both image transformation techniques for <strong>data augmentation</strong> help the model learn more effectively.<br>
<img src="2022-03-23-11-26-47.png" alt></p>
<h3 class="mume-header" id="table-detection-evaluation">Table detection evaluation</h3>

<p>First, we fine-tune Cascade mask R-CNN HRNet on the <strong>ICDAR 19</strong> track A train set along with dilation transform augmentation, and the following results were obtained on the modern tack A test set.<br>
<img src="2022-03-23-11-27-28.png" alt></p>
<p>Evaluation metrics for <strong>TableBank</strong> dataset for table detection are based on, calculating the Precision, Recall, and F1.<br>
<img src="2022-03-23-11-27-36.png" alt></p>
<p>Evaluation metrics for <strong>ICDAR 2013</strong> is based on completeness and purity of the sub-objects of a table.<br>
<img src="2022-03-23-11-27-55.png" alt></p>
<h3 class="mume-header" id="table-structure-recognition-evaluation">Table structure recognition evaluation</h3>

<p>For each cell, it is required to return the coordinates of a polygon defining the convex hull of the <strong>cell&#x2019;s contents</strong>.<br>
<img src="2022-03-23-13-55-00.png" alt></p>
<p>It predicts accurate cell masks for most of the borderless tables. For some images where some of the predictions for cells are missed by the model (5 c.), we correct it using line estimation and contour-based text detection algorithm. The model fails badly for some images (5 d.).<br>
<img src="2022-03-23-14-48-20.png" alt></p>

      </div>
      
      
    
    
    
    
    
    
    
    
  
    </body></html>